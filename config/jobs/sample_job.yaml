name: sample_csv_to_parquet
description: Sample ETL job that reads CSV, applies transformations, and writes to Parquet

extract:
  type: csv
  options:
    path: data/input/sample.csv
    header: true
    inferSchema: true

transform:
  - type: column
    options:
      type: rename
      columns:
        old_name: new_name
        customer_id: id
  
  - type: filter
    options:
      condition: "age > 18"
  
  - type: column
    options:
      type: add_column
      column_name: processed_date
      expression: "current_date()"

load:
  type: parquet
  options:
    path: data/output/processed_data
    mode: overwrite
    partition_by: 
      - processed_date

spark_config:
  app_name: SampleETLJob
  master: local[*]
  configs:
    spark.sql.shuffle.partitions: 10
    spark.executor.memory: 2g 